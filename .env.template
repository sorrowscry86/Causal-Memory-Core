# Causal Memory Core - Environment Configuration Template
# Copy this file to .env and configure your settings

# =============================================================================
# REQUIRED CONFIGURATION
# =============================================================================

# OpenAI API Key (REQUIRED)
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# =============================================================================
# DATABASE CONFIGURATION
# =============================================================================

# Database file path (default: causal_memory.db)
# Use absolute path for production deployments
DB_PATH=causal_memory.db

# =============================================================================
# LLM CONFIGURATION
# =============================================================================

# OpenAI model to use for causal analysis
# Options: gpt-3.5-turbo, gpt-4, gpt-4-turbo
LLM_MODEL=gpt-3.5-turbo

# Temperature for LLM responses (0.0-1.0)
# Lower values = more deterministic responses
LLM_TEMPERATURE=0.1

# =============================================================================
# EMBEDDING MODEL CONFIGURATION
# =============================================================================

# Sentence transformer model for semantic embeddings
# Options: all-MiniLM-L6-v2 (default, fast), all-mpnet-base-v2 (slower, more accurate)
EMBEDDING_MODEL=all-MiniLM-L6-v2

# =============================================================================
# SEARCH AND ANALYSIS CONFIGURATION
# =============================================================================

# Maximum number of potential causes to consider for each event
# Higher values = more thorough analysis but slower processing
MAX_POTENTIAL_CAUSES=5

# Similarity threshold for causal relationship detection (0.0-1.0)
# Higher values = more strict matching
SIMILARITY_THRESHOLD=0.5

# Time decay window in hours for causal analysis
# Events older than this are less likely to be considered as causes
TIME_DECAY_HOURS=24

# Narrative Chain Configuration
# MAX_CONSEQUENCE_DEPTH: How many consequence events to include in narrative chains
# Default is 2. Increase for longer forward-looking narratives, decrease to 0 for causes-only.
MAX_CONSEQUENCE_DEPTH=2

# HTTP API Server Configuration
PORT=8000
HOST=0.0.0.0

# API Security (Optional)
# Uncomment to require API key authentication
# API_KEY=your-secret-api-key-here

# CORS Configuration (comma-separated list of allowed origins)
# Use "*" to allow all origins (default)
CORS_ORIGINS=*

# =============================================================================
# MCP SERVER CONFIGURATION
# =============================================================================

# MCP Server name for identification
MCP_SERVER_NAME=causal-memory-core

# MCP Server version
MCP_SERVER_VERSION=1.1.1

# =============================================================================
# PREPROCESSOR CONFIGURATION (ADVANCED)
# =============================================================================

# Enable semantic query preprocessing (experimental feature)
# Options: true, false
PREPROCESSOR_ENABLED=false

# Fail-open behavior for preprocessor errors
# Options: true, false
PREPROCESSOR_FAIL_OPEN=true

# Minimum confidence threshold for semantic translation
# Options: 0.0-1.0
PREPROCESSOR_CONFIDENCE_THRESHOLD=0.6

# Enable debug tool exposure in MCP (development only)
# Options: true, false
PREPROCESSOR_DEBUG_ENABLED=false

# Limit for recent translations stored in memory
PREPROCESSOR_METRICS_RECENT_LIMIT=50

# =============================================================================
# SUGGESTIONS CONFIGURATION (EXPERIMENTAL)
# =============================================================================

# Enable query suggestions (experimental feature)
# Options: true, false
PREPROCESSOR_SUGGESTIONS_ENABLED=false

# Number of top suggestions to provide
PREPROCESSOR_SUGGESTION_TOP_K=3

# =============================================================================
# PRODUCTION DEPLOYMENT CONFIGURATION
# =============================================================================

# Uncomment and configure for production deployments

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
# LOG_LEVEL=INFO

# Enable structured logging for production monitoring
# STRUCTURED_LOGGING=true

# Database connection pool size (for high-traffic deployments)
# DB_POOL_SIZE=10

# API rate limiting (requests per minute)
# RATE_LIMIT_PER_MINUTE=100

# Maximum query length in characters
# MAX_QUERY_LENGTH=1000

# Maximum event description length in characters
# MAX_EVENT_DESCRIPTION_LENGTH=10000

# =============================================================================
# DOCKER CONFIGURATION
# =============================================================================

# Uncomment and configure for Docker deployments

# Docker container restart policy
# RESTART_POLICY=unless-stopped

# Docker volume mount path for persistent data
# DOCKER_DATA_PATH=/app/data

# Container resource limits
# MEMORY_LIMIT=512m
# CPU_LIMIT=1.0

# =============================================================================
# SECURITY CONFIGURATION
# =============================================================================

# API authentication token (for production deployments)
# API_AUTH_TOKEN=your_secure_token_here

# Enable HTTPS for API endpoints (requires SSL certificates)
# ENABLE_HTTPS=false

# CORS origins for web API access
# CORS_ORIGINS=http://localhost:3000,https://yourdomain.com

# =============================================================================
# MONITORING AND OBSERVABILITY
# =============================================================================

# Enable metrics collection
# ENABLE_METRICS=true

# Metrics export endpoint
# METRICS_ENDPOINT=/metrics

# Health check endpoint
# HEALTH_CHECK_ENDPOINT=/health

# Enable performance profiling
# ENABLE_PROFILING=false

# =============================================================================
# DEVELOPMENT CONFIGURATION
# =============================================================================

# Enable debug mode (development only)
# DEBUG=false

# Enable verbose logging (development only)
# VERBOSE_LOGGING=false

# Test database path (for running tests)
# TEST_DB_PATH=test_causal_memory.db

# =============================================================================
# EXAMPLE CONFIGURATIONS
# =============================================================================

# Development Configuration:
# OPENAI_API_KEY=your_key_here
# DB_PATH=causal_memory.db
# LLM_MODEL=gpt-3.5-turbo
# SIMILARITY_THRESHOLD=0.5
# DEBUG=true

# Production Configuration:
# OPENAI_API_KEY=your_key_here
# DB_PATH=/app/data/causal_memory.db
# LLM_MODEL=gpt-4
# SIMILARITY_THRESHOLD=0.7
# LOG_LEVEL=INFO
# STRUCTURED_LOGGING=true
# API_AUTH_TOKEN=your_secure_token_here
# ENABLE_METRICS=true

# High-Performance Configuration:
# OPENAI_API_KEY=your_key_here
# DB_PATH=/app/data/causal_memory.db
# LLM_MODEL=gpt-4-turbo
# MAX_POTENTIAL_CAUSES=10
# SIMILARITY_THRESHOLD=0.6
# DB_POOL_SIZE=20
# RATE_LIMIT_PER_MINUTE=500
# ENABLE_METRICS=true
