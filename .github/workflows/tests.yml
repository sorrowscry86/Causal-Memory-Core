name: 🧪 Comprehensive Test Suite

on:
  push:
    branches: [ main, develop, feature/* ]
  pull_request:
    branches: [ main, develop ]

env:
  PYTHON_VERSION: "3.11"
  # Mock OpenAI API key for tests (tests should mock all external calls)
  OPENAI_API_KEY: "sk-test-mock-key-for-testing-only"

jobs:
  test:
    name: 🧪 Test Suite (Python ${{ matrix.python-version }})
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        # Focus on Python 3.11 but include 3.9, 3.10 for compatibility
        # Skip 3.12/3.13 due to pydantic-core build issues
        python-version: ["3.9", "3.10", "3.11"]
    
    steps:
    - name: 📥 Checkout code
      uses: actions/checkout@v5

    - name: 🐍 Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}

    - name: 📦 Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ matrix.python-version }}-
          ${{ runner.os }}-pip-

    - name: 🔧 Install dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel
        pip install -r requirements.txt
        # Install essential testing packages
        pip install pytest pytest-cov pytest-html pytest-mock coverage[toml]

    - name: 🧪 Run comprehensive test suite
      run: |
        # Create test results directory
        mkdir -p test_results/artifacts
        
        # Run unit tests with coverage
        echo "🔬 Running unit tests..."
        python -m pytest tests/test_*.py -v \
          --cov=src \
          --cov-report=term-missing \
          --cov-report=xml:test_results/coverage.xml \
          --cov-report=html:test_results/htmlcov \
          --html=test_results/unit_tests_report.html \
          --self-contained-html \
          --junitxml=test_results/unit_tests_junit.xml
        
        # Run E2E tests
        echo "🔗 Running E2E tests..."
        python -m pytest tests/e2e/ -v \
          --html=test_results/e2e_tests_report.html \
          --self-contained-html \
          --junitxml=test_results/e2e_tests_junit.xml
          
        # Run categorized tests 
        echo "📊 Running categorized tests..."
        python -m pytest tests -m "unit" -v --tb=short || true
        python -m pytest tests -m "e2e" -v --tb=short || true
      env:
        OPENAI_API_KEY: ${{ env.OPENAI_API_KEY }}

    - name: 🚀 Run comprehensive test scripts
      run: |
        # Run the comprehensive test runner
        echo "🎯 Running comprehensive test script..."
        python run_comprehensive_tests.py || true
        
        # Run E2E test script
        echo "🔄 Running E2E test script..."
        python run_e2e_tests.py --all || true
      env:
        OPENAI_API_KEY: ${{ env.OPENAI_API_KEY }}

    - name: 📊 Generate test summary
      if: always()
      run: |
        echo "## 🧪 Test Results Summary for Python ${{ matrix.python-version }}" >> test_results/summary.md
        echo "**Timestamp:** $(date -u)" >> test_results/summary.md
        echo "**Python Version:** ${{ matrix.python-version }}" >> test_results/summary.md
        echo "**OS:** ubuntu-latest" >> test_results/summary.md
        echo "" >> test_results/summary.md
        
        # Count test results from JUnit XML if available
        if [ -f test_results/unit_tests_junit.xml ]; then
          echo "### Unit Tests" >> test_results/summary.md
          python -c "
import xml.etree.ElementTree as ET
try:
    tree = ET.parse('test_results/unit_tests_junit.xml')
    root = tree.getroot()
    tests = root.get('tests', '0')
    failures = root.get('failures', '0') 
    errors = root.get('errors', '0')
    skipped = root.get('skipped', '0')
    time = root.get('time', '0')
    print(f'- **Total:** {tests}')
    print(f'- **Passed:** {int(tests) - int(failures) - int(errors) - int(skipped)}')
    print(f'- **Failed:** {failures}')
    print(f'- **Errors:** {errors}')
    print(f'- **Skipped:** {skipped}')
    print(f'- **Duration:** {time}s')
except Exception as e:
    print(f'- **Status:** Error parsing results - {e}')
" >> test_results/summary.md
        fi
        
        if [ -f test_results/e2e_tests_junit.xml ]; then
          echo "" >> test_results/summary.md
          echo "### E2E Tests" >> test_results/summary.md
          python -c "
import xml.etree.ElementTree as ET
try:
    tree = ET.parse('test_results/e2e_tests_junit.xml')
    root = tree.getroot()
    tests = root.get('tests', '0')
    failures = root.get('failures', '0')
    errors = root.get('errors', '0')
    skipped = root.get('skipped', '0')
    time = root.get('time', '0')
    print(f'- **Total:** {tests}')
    print(f'- **Passed:** {int(tests) - int(failures) - int(errors) - int(skipped)}')
    print(f'- **Failed:** {failures}')
    print(f'- **Errors:** {errors}')
    print(f'- **Skipped:** {skipped}')
    print(f'- **Duration:** {time}s')
except Exception as e:
    print(f'- **Status:** Error parsing results - {e}')
" >> test_results/summary.md
        fi
        
        echo "" >> test_results/summary.md
        echo "### Coverage" >> test_results/summary.md
        if [ -f test_results/coverage.xml ]; then
          python -c "
import xml.etree.ElementTree as ET
try:
    tree = ET.parse('test_results/coverage.xml')
    root = tree.getroot()
    line_rate = root.get('line-rate', '0')
    coverage_pct = float(line_rate) * 100
    print(f'- **Line Coverage:** {coverage_pct:.1f}%')
except Exception as e:
    print(f'- **Coverage:** Error parsing coverage - {e}')
" >> test_results/summary.md
        fi
        
        # Display summary
        cat test_results/summary.md

    - name: 📤 Upload test artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-results-python-${{ matrix.python-version }}
        path: |
          test_results/
          !test_results/**/*.pyc
        retention-days: 30

    - name: 📋 Upload coverage to Codecov
      uses: codecov/codecov-action@v4
      if: matrix.python-version == '3.11' && always()
      with:
        file: test_results/coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

  test-matrix-summary:
    name: 📊 Test Matrix Summary  
    runs-on: ubuntu-latest
    needs: test
    if: always()
    
    steps:
    - name: 📥 Download all test artifacts
      uses: actions/download-artifact@v4
      with:
        path: all-test-results/

    - name: 📋 Generate matrix summary
      run: |
        echo "# 🧪 Comprehensive Test Matrix Results" > matrix_summary.md
        echo "" >> matrix_summary.md
        echo "| Python Version | Status | Test Results |" >> matrix_summary.md  
        echo "|----------------|--------|--------------|" >> matrix_summary.md
        
        for version in 3.9 3.10 3.11; do
          if [ -d "all-test-results/test-results-python-${version}" ]; then
            if [ -f "all-test-results/test-results-python-${version}/summary.md" ]; then
              echo "| ${version} | ✅ Completed | [Summary](./test-results-python-${version}/summary.md) |" >> matrix_summary.md
            else
              echo "| ${version} | ❌ Failed | No summary available |" >> matrix_summary.md
            fi
          else
            echo "| ${version} | ❌ Failed | No artifacts |" >> matrix_summary.md
          fi
        done
        
        echo "" >> matrix_summary.md
        echo "## 📦 Available Artifacts" >> matrix_summary.md
        echo "- HTML Test Reports" >> matrix_summary.md
        echo "- Coverage Reports (XML/HTML)" >> matrix_summary.md
        echo "- JUnit XML Results" >> matrix_summary.md
        echo "- Individual Python version summaries" >> matrix_summary.md
        
        # Display the summary
        cat matrix_summary.md

    - name: 📤 Upload matrix summary
      uses: actions/upload-artifact@v4
      with:
        name: test-matrix-summary
        path: |
          matrix_summary.md
          all-test-results/
        retention-days: 30

  performance-validation:
    name: ⚡ Performance Validation
    runs-on: ubuntu-latest
    needs: test
    if: contains(fromJSON('["3.11"]'), needs.test.result) != false
    
    steps:
    - name: 📥 Checkout code
      uses: actions/checkout@v5
      
    - name: 🐍 Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        
    - name: 🔧 Install dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel
        pip install -r requirements.txt
        pip install psutil
        
    - name: 🏃‍♂️ Run performance benchmarks
      run: |
        echo "🚀 Running performance benchmarks..."
        python quick_benchmark.py || true
      env:
        OPENAI_API_KEY: ${{ env.OPENAI_API_KEY }}

  security-scan:
    name: 🔒 Security Scan
    runs-on: ubuntu-latest
    
    steps:
    - name: 📥 Checkout code
      uses: actions/checkout@v5
      
    - name: 🐍 Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        
    - name: 🔧 Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install safety bandit[toml]
        
    - name: 🛡️ Run safety check
      run: |
        echo "🔍 Checking for known security vulnerabilities..."
        safety check --ignore 70612 || true
        
    - name: 🔍 Run bandit security linter
      run: |
        echo "🔐 Running bandit security scan..."
        bandit -r src/ -f json -o bandit-report.json || true
        bandit -r src/ -f txt || true
        
    - name: 📤 Upload security reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-reports
        path: bandit-report.json
        retention-days: 30